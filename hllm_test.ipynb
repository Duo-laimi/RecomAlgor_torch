{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0463c442-dff6-49cb-aead-8b50d3970076",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T04:04:04.763529Z",
     "iopub.status.busy": "2026-01-29T04:04:04.763439Z",
     "iopub.status.idle": "2026-01-29T04:04:09.382228Z",
     "shell.execute_reply": "2026-01-29T04:04:09.381745Z",
     "shell.execute_reply.started": "2026-01-29T04:04:04.763516Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;35mGeneral Hyper Parameters:\n",
      "\u001b[0m\u001b[1;36mmodel\u001b[0m =\u001b[1;33m HLLM\u001b[0m\n",
      "\u001b[1;36mseed\u001b[0m =\u001b[1;33m 2020\u001b[0m\n",
      "\u001b[1;36mstate\u001b[0m =\u001b[1;33m INFO\u001b[0m\n",
      "\u001b[1;36muse_text\u001b[0m =\u001b[1;33m True\u001b[0m\n",
      "\u001b[1;36mreproducibility\u001b[0m =\u001b[1;33m True\u001b[0m\n",
      "\u001b[1;36mcheckpoint_dir\u001b[0m =\u001b[1;33m saved\u001b[0m\n",
      "\u001b[1;36mshow_progress\u001b[0m =\u001b[1;33m True\u001b[0m\n",
      "\u001b[1;36mlog_wandb\u001b[0m =\u001b[1;33m False\u001b[0m\n",
      "\u001b[1;36mdata_path\u001b[0m =\u001b[1;33m /mnt/workspace/dataset/hllm/dataset\u001b[0m\n",
      "\u001b[1;36mstrategy\u001b[0m =\u001b[1;33m deepspeed\u001b[0m\n",
      "\u001b[1;36mprecision\u001b[0m =\u001b[1;33m bf16-mixed\u001b[0m\n",
      "\n",
      "\u001b[1;35mTraining Hyper Parameters:\n",
      "\u001b[0m\u001b[1;36mepochs\u001b[0m =\u001b[1;33m 10\u001b[0m\n",
      "\u001b[1;36mtrain_batch_size\u001b[0m =\u001b[1;33m 2\u001b[0m\n",
      "\u001b[1;36moptim_args\u001b[0m =\u001b[1;33m {'learning_rate': 0.0001, 'weight_decay': 0.01}\u001b[0m\n",
      "\u001b[1;36meval_step\u001b[0m =\u001b[1;33m 1\u001b[0m\n",
      "\u001b[1;36mstopping_step\u001b[0m =\u001b[1;33m 5\u001b[0m\n",
      "\n",
      "\u001b[1;35mEvaluation Hyper Parameters:\n",
      "\u001b[0m\u001b[1;36meval_batch_size\u001b[0m =\u001b[1;33m 256\u001b[0m\n",
      "\u001b[1;36mtopk\u001b[0m =\u001b[1;33m [5, 10, 50, 200]\u001b[0m\n",
      "\u001b[1;36mmetrics\u001b[0m =\u001b[1;33m ['Recall', 'NDCG']\u001b[0m\n",
      "\u001b[1;36mvalid_metric\u001b[0m =\u001b[1;33m NDCG@200\u001b[0m\n",
      "\u001b[1;36mmetric_decimal_place\u001b[0m =\u001b[1;33m 7\u001b[0m\n",
      "\u001b[1;36meval_type\u001b[0m =\u001b[1;33m EvaluatorType.RANKING\u001b[0m\n",
      "\u001b[1;36mvalid_metric_bigger\u001b[0m =\u001b[1;33m True\u001b[0m\n",
      "\n",
      "\u001b[1;35mDataset Hyper Parameters:\n",
      "\u001b[0m\u001b[1;36mMAX_ITEM_LIST_LENGTH\u001b[0m =\u001b[1;33m 50\u001b[0m\n",
      "\u001b[1;36mMAX_TEXT_LENGTH\u001b[0m =\u001b[1;33m 64\u001b[0m\n",
      "\u001b[1;36mtext_keys\u001b[0m =\u001b[1;33m ['title', 'tag', 'description']\u001b[0m\n",
      "\u001b[1;36mitem_prompt\u001b[0m =\u001b[1;33m Compress the following sentence into embedding: \u001b[0m\n",
      "\n",
      "\u001b[1;35mOther Hyper Parameters: \n",
      "\u001b[0m\u001b[1;36mitem_pretrain_dir\u001b[0m = \u001b[1;33m/mnt/workspace/model/TinyLlama-1.1B\u001b[0m\n",
      "\u001b[1;36mitem_llm_init\u001b[0m = \u001b[1;33mTrue\u001b[0m\n",
      "\u001b[1;36muser_pretrain_dir\u001b[0m = \u001b[1;33m/mnt/workspace/model/TinyLlama-1.1B\u001b[0m\n",
      "\u001b[1;36muser_llm_init\u001b[0m = \u001b[1;33mTrue\u001b[0m\n",
      "\u001b[1;36muse_ft_flash_attn\u001b[0m = \u001b[1;33mTrue\u001b[0m\n",
      "\u001b[1;36mwandb_project\u001b[0m = \u001b[1;33mREC\u001b[0m\n",
      "\u001b[1;36mtext_path\u001b[0m = \u001b[1;33m/mnt/workspace/dataset/hllm/information/Pixel200K.csv\u001b[0m\n",
      "\u001b[1;36mitem_emb_token_n\u001b[0m = \u001b[1;33m1\u001b[0m\n",
      "\u001b[1;36mloss\u001b[0m = \u001b[1;33mnce\u001b[0m\n",
      "\u001b[1;36mscheduler_args\u001b[0m = \u001b[1;33m{'type': 'cosine', 'warmup': 0.1}\u001b[0m\n",
      "\u001b[1;36mstage\u001b[0m = \u001b[1;33m2\u001b[0m\n",
      "\u001b[1;36mMODEL_INPUT_TYPE\u001b[0m = \u001b[1;33mInputType.SEQ\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"12500\"\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "dist.init_process_group(backend='nccl')\n",
    "config_list = [\n",
    "    \"assets/HLLM/HLLM.yaml\",\n",
    "    \"assets/overall/LLM_deepspeed.yaml\"\n",
    "]\n",
    "from config.configurator import Config\n",
    "cfg = Config(config_list)\n",
    "print(cfg)\n",
    "# torch.manual_seed(cfg.get(\"seed\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004007f8-28a4-4067-9999-24400222731b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T04:04:20.145778Z",
     "iopub.status.busy": "2026-01-29T04:04:20.145596Z",
     "iopub.status.idle": "2026-01-29T04:04:20.163253Z",
     "shell.execute_reply": "2026-01-29T04:04:20.162827Z",
     "shell.execute_reply.started": "2026-01-29T04:04:20.145760Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_rank = 0\n",
    "device = torch.device(\"cuda\", local_rank)\n",
    "cfg['device'] = device\n",
    "extra_args_str = \\\n",
    "\"\"\"\n",
    "--loss nce\n",
    "--epochs 5\n",
    "--dataset Pixel200K\n",
    "--train_batch_size 16\n",
    "--MAX_TEXT_LENGTH 256\n",
    "--MAX_ITEM_LIST_LENGTH 10\n",
    "--checkpoint_dir saved_path\n",
    "--optim_args.learning_rate 1e-4\n",
    "--item_pretrain_dir item_pretrain_dir\n",
    "--user_pretrain_dir user_pretrain_dir\n",
    "--text_path text_path\n",
    "--text_keys [\\\"title\\\", \\\"tag\\\", \\\"description\\\"]\n",
    "\"\"\"\n",
    "extra_args_list = extra_args_str.strip().split(\"\\n\")\n",
    "extra_args_list = [args.split(\" \", maxsplit=1) for args in extra_args_list]\n",
    "extra_args = []\n",
    "for args_list in extra_args_list:\n",
    "    extra_args.extend(args_list)\n",
    "# cfg.parse_extra_args(extra_args)\n",
    "\n",
    "from utils import init_seed\n",
    "init_seed(cfg['seed'], cfg['reproducibility'])\n",
    "\n",
    "# dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e934f9-26f4-4191-8d2b-25fd00d0f24e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T04:04:23.393915Z",
     "iopub.status.busy": "2026-01-29T04:04:23.393738Z",
     "iopub.status.idle": "2026-01-29T04:04:37.790032Z",
     "shell.execute_reply": "2026-01-29T04:04:37.789439Z",
     "shell.execute_reply.started": "2026-01-29T04:04:23.393899Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "\u001b[1;35mPixel200K\u001b[0m\n",
      "\u001b[1;34mThe number of users\u001b[0m: 200001\n",
      "\u001b[1;34mAverage actions of users\u001b[0m: 19.82828\n",
      "\u001b[1;34mThe number of items\u001b[0m: 96283\n",
      "\u001b[1;34mAverage actions of items\u001b[0m: 41.187927130720176\n",
      "\u001b[1;34mThe number of inters\u001b[0m: 3965656\n",
      "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.9794063532928%\n"
     ]
    }
   ],
   "source": [
    "from data import load_data\n",
    "\n",
    "dataload = load_data(cfg)\n",
    "\n",
    "from data import bulid_dataloader\n",
    "\n",
    "train_loader, valid_loader, test_loader = bulid_dataloader(cfg, dataload)\n",
    "\n",
    "print(dataload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52df055f-149c-4c1e-93e1-6f57ebbc47b0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T04:05:07.259478Z",
     "iopub.status.busy": "2026-01-29T04:05:07.259269Z",
     "iopub.status.idle": "2026-01-29T04:05:07.494306Z",
     "shell.execute_reply": "2026-01-29T04:05:07.493085Z",
     "shell.execute_reply.started": "2026-01-29T04:05:07.259460Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:41: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:41: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:41: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [2, 51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:41: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [2, 51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [2, 51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:41: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:41: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [2, 50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [2, 50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [2, 51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [612], which does not match the required output shape [2, 51, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [2, 50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [2, 51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [2, 51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [612], which does not match the required output shape [2, 51, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [612], which does not match the required output shape [2, 51, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [2, 50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [2, 50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [612], which does not match the required output shape [2, 51, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [2, 50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [612], which does not match the required output shape [2, 51, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [612], which does not match the required output shape [2, 51, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:41: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:41: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [2, 51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [102], which does not match the required output shape [2, 51]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [2, 50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [2, 50]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [612], which does not match the required output shape [2, 51, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/mnt/workspace/code/RecomAlgor_torch/data/dataset/collate_fn.py:43: UserWarning: An output with one or more elements was resized since it had shape [612], which does not match the required output shape [2, 51, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  return torch.stack(batch, 0, out=out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos_item_ids': tensor([[54961, 71171, 20766, 80933, 12016, 60008, 31466, 27471, 53705, 87849,\n",
      "         26910, 79237, 19517, 61800, 59291, 13761, 92493, 87832, 21196, 64375,\n",
      "         81476, 70445, 62330,  5355, 58035,  6468, 45213, 28445, 15939, 69716,\n",
      "         90774, 12262, 80347, 34469, 20374,  3443, 84371, 70657, 39371, 60806,\n",
      "         31117, 40260, 10038, 49073, 10730, 59035,   254, 57752,  6383, 20356,\n",
      "          8219],\n",
      "        [47924, 29724, 78618,  3533, 52689, 87067, 16260, 36379, 67151, 43348,\n",
      "         80382,  4177, 26698,  7122,  6055, 21510, 27899, 21784, 35018, 10500,\n",
      "         11981, 33353, 30747, 12265,  7507,   683, 49077, 21027, 10717, 29542,\n",
      "         40836,  6579, 28526,  4406, 35919, 10020, 23335, 11144, 54420, 68702,\n",
      "         23400, 13725, 22471, 29782,  5140, 57207,  2910, 57112, 10475, 47002,\n",
      "         40053]]), 'neg_item_ids': tensor([[65725, 84049, 82029, 60269, 32074, 67555,  5365, 80792, 68473, 81702,\n",
      "         88669, 31532, 96191, 32186, 74597, 38574, 38829, 16793, 95086, 55439,\n",
      "         50654, 81384, 38035, 21234, 93295, 58895, 45779, 82327, 13028, 11653,\n",
      "         30785, 74595, 31418, 72639, 71012, 65641, 83363,  5905, 65649, 12823,\n",
      "         18227,  7587, 46265, 81222, 81198, 22876, 87871, 60427, 57998, 62150,\n",
      "         48319],\n",
      "        [58451, 45956, 11801, 69528, 16786, 94676, 53979,  3765,  1745, 65443,\n",
      "         18440, 70140, 31890, 73384, 59929, 92420, 13592, 55661, 82075, 29886,\n",
      "         64358, 44509, 88563, 57878, 10532, 26516, 41130, 45601,  3778, 68124,\n",
      "         62537, 78672, 82482, 80636, 77894, 84676, 69646, 52623, 51414, 86308,\n",
      "         33185, 87177, 93720, 83671,  8781, 41355, 14539, 62761, 78737, 51764,\n",
      "         70218]]), 'pos_input_ids': tensor([    1,   422,  2139,  ...,  1307, 29999,     0]), 'pos_cu_input_lens': tensor([65, 65, 65, 65, 65, 65, 38, 65, 58, 65, 65, 36, 31, 31, 28, 35, 65, 65,\n",
      "        65, 65, 65, 46, 65, 65, 65, 65, 53, 65, 65, 65, 65, 65, 62, 53, 31, 65,\n",
      "        50, 52, 59, 52, 65, 34, 65, 65, 42, 45, 65, 65, 42, 63, 30, 45, 32, 65,\n",
      "        42, 65, 64, 65, 27, 62, 56, 54, 65, 65, 65, 65, 65, 44, 34, 65, 65, 65,\n",
      "        65, 41, 65, 65, 65, 65, 56, 57, 37, 65, 50, 65, 61, 65, 53, 41, 26, 59,\n",
      "        39, 36, 65, 65, 21, 65, 44, 41, 41, 65, 35, 61]), 'pos_position_ids': tensor([ 0,  1,  2,  ..., 62, 63, 64]), 'neg_input_ids': tensor([    1,   422,  2139,  ..., 29914,  8419,     0]), 'neg_cu_input_lens': tensor([54, 65, 65, 65, 65, 38, 65, 42, 65, 65, 30, 65, 65, 65, 41, 38, 65, 38,\n",
      "        65, 59, 65, 65, 59, 36, 50, 65, 50, 52, 65, 65, 44, 65, 65, 65, 51, 51,\n",
      "        65, 65, 65, 48, 53, 65, 65, 65, 56, 56, 46, 38, 29, 44, 65, 65, 65, 65,\n",
      "        65, 65, 58, 49, 31, 65, 51, 65, 58, 65, 65, 43, 65, 65, 65, 47, 40, 45,\n",
      "        55, 58, 56, 65, 65, 18, 31, 48, 65, 38, 65, 65, 65, 44, 35, 65, 48, 65,\n",
      "        58, 65, 52, 53, 43, 65, 43, 65, 38, 47, 39, 44]), 'neg_position_ids': tensor([11, 12, 13,  ..., 62, 63, 64]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]), 'time_ids': tensor([[[1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [2019,    7,   28,   10,   57,   55],\n",
      "         [2020,    2,    8,    1,   41,   59],\n",
      "         [2020,    4,   14,   13,   19,   47],\n",
      "         [2020,    4,   29,   11,    8,   23],\n",
      "         [2020,    8,    8,    1,   43,   48],\n",
      "         [2020,   11,   25,    5,   25,   25],\n",
      "         [2020,   12,    2,   11,   32,   13],\n",
      "         [2021,    4,    9,   23,   28,   55],\n",
      "         [2021,    5,    4,    9,   26,   58]],\n",
      "\n",
      "        [[1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [1970,    1,    1,    0,    0,    0],\n",
      "         [2020,    7,   28,   17,   11,    0],\n",
      "         [2020,    8,    3,   15,   52,   58],\n",
      "         [2020,    8,   21,    0,    7,   30],\n",
      "         [2020,   10,    1,    3,   13,   54],\n",
      "         [2021,    6,   15,    9,   34,    3],\n",
      "         [2021,    6,   18,   13,   19,   42],\n",
      "         [2021,    7,   19,    6,   34,   20],\n",
      "         [2021,    8,    2,   15,   51,   16],\n",
      "         [2021,    8,   15,   13,   28,   32],\n",
      "         [2021,    8,   19,    3,   45,   25],\n",
      "         [2021,    8,   21,    8,   31,   52],\n",
      "         [2021,    8,   26,   13,   57,    6],\n",
      "         [2021,    8,   27,    7,   32,   18],\n",
      "         [2021,    8,   28,    7,   58,    0],\n",
      "         [2021,    8,   31,    7,   29,   24],\n",
      "         [2021,    9,    1,    9,   40,   30],\n",
      "         [2021,    9,    2,   13,   47,    7],\n",
      "         [2021,    9,   12,   22,   22,   31],\n",
      "         [2021,    9,   28,   23,   11,    2],\n",
      "         [2021,   10,    7,    9,   28,   15],\n",
      "         [2021,   10,   17,   15,   24,    3],\n",
      "         [2021,   10,   22,   12,   46,   59],\n",
      "         [2021,   10,   31,   12,   52,   13],\n",
      "         [2021,   11,   13,    1,   41,   14],\n",
      "         [2021,   11,   22,   10,   50,   38],\n",
      "         [2021,   11,   23,    5,   49,   24],\n",
      "         [2021,   11,   26,    5,    0,   31],\n",
      "         [2021,   11,   26,   13,   33,   37],\n",
      "         [2021,   12,    4,    6,   40,   26],\n",
      "         [2021,   12,    7,   23,   38,   53],\n",
      "         [2021,   12,   11,   23,   21,    7],\n",
      "         [2021,   12,   17,    4,   49,    1],\n",
      "         [2021,   12,   24,    0,   58,   14],\n",
      "         [2021,   12,   24,    1,    0,   30],\n",
      "         [2021,   12,   25,   12,   19,   16],\n",
      "         [2022,    1,   22,    0,    3,   27],\n",
      "         [2022,    1,   26,   12,   21,   26],\n",
      "         [2022,    1,   29,   16,   45,   39],\n",
      "         [2022,    2,   18,   12,    6,   51],\n",
      "         [2022,    2,   21,    3,   53,   53]]])}\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_loader)\n",
    "first_bt = next(it)\n",
    "print(first_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "269d3a91-559a-405f-8756-88520e89475d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T04:21:22.373233Z",
     "iopub.status.busy": "2026-01-29T04:21:22.373072Z",
     "iopub.status.idle": "2026-01-29T04:21:22.605294Z",
     "shell.execute_reply": "2026-01-29T04:21:22.604793Z",
     "shell.execute_reply.started": "2026-01-29T04:21:22.373217Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[rank0]:[W129 12:21:22.237742241 ProcessGroupNCCL.cpp:5072] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()\n"
     ]
    }
   ],
   "source": [
    "import logging as log\n",
    "from utils.logger import init_logger\n",
    "init_logger(cfg)\n",
    "logger = log.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c1b300-3b0f-4ed9-b0c3-0cb2f95172a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T04:21:27.329905Z",
     "iopub.status.busy": "2026-01-29T04:21:27.329734Z",
     "iopub.status.idle": "2026-01-29T04:21:27.641273Z",
     "shell.execute_reply": "2026-01-29T04:21:27.640814Z",
     "shell.execute_reply.started": "2026-01-29T04:21:27.329889Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 Jan 12:21    INFO  create item llm\n",
      "29 Jan 12:21    INFO  ******* create LLM /mnt/workspace/model/TinyLlama-1.1B *******\n",
      "29 Jan 12:21    INFO  hf_config: LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.6\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n",
      "29 Jan 12:21    INFO  xxxxx starting loading checkpoint\n",
      "29 Jan 12:21    INFO  Using flash attention True for llama\n",
      "29 Jan 12:21    INFO  Init True for llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 Jan 12:21    INFO  create user llm\n",
      "29 Jan 12:21    INFO  ******* create LLM /mnt/workspace/model/TinyLlama-1.1B *******\n",
      "29 Jan 12:21    INFO  hf_config: LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.6\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n",
      "29 Jan 12:21    INFO  xxxxx starting loading checkpoint\n",
      "29 Jan 12:21    INFO  Using flash attention True for llama\n",
      "29 Jan 12:21    INFO  Init True for llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 Jan 12:21    INFO  nce thres setting to 0.99\n"
     ]
    }
   ],
   "source": [
    "from utils import get_model\n",
    "model = get_model(cfg['model'])(cfg, dataload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63f0c62f-fb0b-4515-a8ec-50a3eee524f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T04:24:55.089329Z",
     "iopub.status.busy": "2026-01-29T04:24:55.089167Z",
     "iopub.status.idle": "2026-01-29T04:24:55.361994Z",
     "shell.execute_reply": "2026-01-29T04:24:55.361515Z",
     "shell.execute_reply.started": "2026-01-29T04:24:55.089313Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrainer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[32m      2\u001b[39m world_size = torch.distributed.get_world_size()\n\u001b[32m      3\u001b[39m trainer = Trainer(cfg, model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/workspace/code/RecomAlgor_torch/trainer/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mTrainer\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/workspace/code/RecomAlgor_torch/trainer/trainer.py:31\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ensure_dir, get_local_time, early_stopping, calculate_valid_score, dict2str, \\\n\u001b[32m     28\u001b[39m     get_tensorboard, set_color, get_gpu_usage, WandbLogger\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlr_scheduler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mL\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfabric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstrategies\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepSpeedStrategy, DDPStrategy\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTrainer\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lightning'"
     ]
    }
   ],
   "source": [
    "from trainer.trainer import Trainer\n",
    "world_size = torch.distributed.get_world_size()\n",
    "trainer = Trainer(cfg, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
